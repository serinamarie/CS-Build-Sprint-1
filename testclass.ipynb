{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperfunctions import train_test_split\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Iris.csv\")\n",
    "df.head()\n",
    "df.drop(columns='Id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((123, 5), (30, 5))"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm          Species\n",
       "82             5.8           2.7            3.9           1.2  Iris-versicolor\n",
       "144            6.7           3.3            5.7           2.5   Iris-virginica\n",
       "47             4.6           3.2            1.4           0.2      Iris-setosa\n",
       "18             5.7           3.8            1.7           0.3      Iris-setosa\n",
       "39             5.1           3.4            1.5           0.2      Iris-setosa"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(min_samples=2, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DecisionTreeClassifier at 0x11d2c5f10>"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DecisionTreeClassifier at 0x11d2c5f10>"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Accuracy:', 1.0)"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    '''Can handle numeric features and dataframes that are free of missing data.\n",
    "    If missing values, please impute first.'''\n",
    "\n",
    "    def __init__(self, min_samples=2, max_depth=5, tree=None):\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "        self.tree = None\n",
    "        \n",
    "    def fit(self, train_df):\n",
    "        \n",
    "        # run method decision_tree and set it as the self.tree attribute\n",
    "        self.tree = self.decision_tree(train_df, self.min_samples, self.max_depth)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def entropy(self, data):\n",
    "\n",
    "        # labels for input data\n",
    "        labels = data[:,-1]\n",
    "\n",
    "        # get unique labels along with their counts\n",
    "        _, label_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "        # array of probabilities of each label\n",
    "        probabilities = label_counts / label_counts.sum()\n",
    "        \n",
    "        # return entropy\n",
    "        return sum(probabilities * -np.log2(probabilities))\n",
    "\n",
    "    def make_classification(self, data):\n",
    "\n",
    "        '''Once the max depth or min samples or purity is 1, \n",
    "        we classify the data with whatever the majority of the labels are'''\n",
    "\n",
    "        # labels for input data\n",
    "        labels = data[:,-1]\n",
    "\n",
    "        # instantiate a Counter object on the labels\n",
    "        counter = Counter(labels)\n",
    "\n",
    "        # return the most common class/label\n",
    "        return counter.most_common(1)[0][0]\n",
    "\n",
    "    def split_data(self, data, split_feature, split_threshold):\n",
    "\n",
    "        # array of only the split_feature\n",
    "        feature_values = data[:,split_feature]\n",
    "\n",
    "        # array where feature values do not exceed threshold, array where does exceed threshold\n",
    "        return data[feature_values <= split_threshold], data[feature_values > split_threshold]\n",
    "\n",
    "    def overall_entropy(self, data_below_threshold, data_above_threshold):\n",
    "\n",
    "        '''Overall entropy'''\n",
    "\n",
    "        p_below = len(data_below_threshold) / (len(data_below_threshold) + len(data_above_threshold))\n",
    "        p_above = len(data_above_threshold) / (len(data_below_threshold) + len(data_above_threshold))\n",
    "\n",
    "        return (p_below * self.entropy(data_below_threshold)) + (p_above * self.entropy(data_above_threshold))\n",
    "\n",
    "    def potential_splits(self, data):\n",
    "        \n",
    "        # dictionary of splits\n",
    "        potential_splits = {}\n",
    "\n",
    "        # store the number of features (not including labels/target)\n",
    "        n_features = len(data[0])\n",
    "\n",
    "        # for each feature in possible features\n",
    "        for feature in range(n_features - 1):\n",
    "\n",
    "            # for our dictionary, each feature should be a key\n",
    "            potential_splits[feature] = []\n",
    "\n",
    "            # we need to iterate through each feature's unique values\n",
    "            unique_values_for_feature = np.unique(data[:, feature])\n",
    "\n",
    "            for index in range(len(unique_values_for_feature)):\n",
    "                \n",
    "                if index != 0:\n",
    "\n",
    "                    # we need to partition the data, we need the midpoint between the unique values\n",
    "                    current = unique_values_for_feature[index]\n",
    "                    prev = unique_values_for_feature[index - 1]\n",
    "                    midpoint = (current + prev) / 2\n",
    "\n",
    "                    # for our dictionary each value should be a midpoint between the \n",
    "                    # unique values for that feature\n",
    "                    potential_splits[feature].append(midpoint)\n",
    "\n",
    "        # return dictionary\n",
    "        return potential_splits\n",
    "\n",
    "    def find_best_split(self, data, potential_splits):\n",
    "\n",
    "        lowest_entropy = 9999\n",
    "\n",
    "        # for each dictionary key\n",
    "        for key in potential_splits:\n",
    "            \n",
    "            # for each value for that key\n",
    "            for value in potential_splits[key]:\n",
    "\n",
    "                # split our data into on that threshold (value)\n",
    "                data_below_threshold, data_above_threshold = self.split_data(\n",
    "                    data=data, \n",
    "                    split_feature=key,\n",
    "                    split_threshold=value)\n",
    "                \n",
    "                # calculate entropy at this split\n",
    "                entropy_for_this_split = self.overall_entropy(data_below_threshold, data_above_threshold)\n",
    "\n",
    "                # if entropy at this split is lower than the lowest entropy found so far\n",
    "                if entropy_for_this_split < lowest_entropy:\n",
    "\n",
    "                    # the entropy at this split is now the lowest \n",
    "                    lowest_entropy = entropy_for_this_split\n",
    "\n",
    "                    # keep a record of this key, value pair\n",
    "                    best_split_feature = key\n",
    "                    best_split_threshold = value\n",
    "\n",
    "        # return the best potential split\n",
    "        return best_split_feature, best_split_threshold\n",
    "    \n",
    "\n",
    "    def purity(self, data):\n",
    "\n",
    "        # last column of the df must be the labels!\n",
    "        labels = data[:, -1]\n",
    "\n",
    "        # if data has only one kind of label\n",
    "        if len(np.unique(labels)) == 1:\n",
    "\n",
    "            # it is pure\n",
    "            return True\n",
    "\n",
    "        # if data has a few different labels still\n",
    "        else:\n",
    "\n",
    "            # it isn't pure\n",
    "            return False\n",
    "\n",
    "\n",
    "    def decision_tree(self, train_df, min_samples, max_depth, counter=0):\n",
    "\n",
    "        '''only one arg needed (df). fitting this training df will account for\n",
    "        splitting data into X and y'''\n",
    "\n",
    "        # if this is our first potential split\n",
    "        if counter == 0:\n",
    "\n",
    "            # set this global variable so we can use it each time we call decision_tree()\n",
    "            global feature_names\n",
    "            feature_names = train_df.columns\n",
    "\n",
    "            # get our df values\n",
    "            data = train_df.values\n",
    "\n",
    "        # if we have recursively reached this point\n",
    "        else:\n",
    "\n",
    "            # our 'train_df' is actually already an array upon recursion\n",
    "            data = train_df\n",
    "            \n",
    "        # base case: if our impurity for a subtree is 0 or we have reached our \n",
    "        # maximum depth or min samples\n",
    "        if (self.purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "            \n",
    "            # at this point we'll have to make a judgment call and classify it based on the majority\n",
    "            return self.make_classification(data)\n",
    "\n",
    "        # if we haven't reach one of our stopping points\n",
    "        else:\n",
    "\n",
    "            # increment counter\n",
    "            counter += 1\n",
    "\n",
    "            # get a dictionary of our potential splits\n",
    "            potential_splits = self.potential_splits(data)\n",
    "            \n",
    "            # find the best split\n",
    "            split_feature, split_threshold = self.find_best_split(data, potential_splits)\n",
    "\n",
    "            # get the data below and above\n",
    "            data_below_threshold, data_above_threshold = self.split_data(data, split_feature, split_threshold)\n",
    "\n",
    "            # store feature name as string\n",
    "            feature_name_as_string = feature_names[split_feature]\n",
    "\n",
    "            # feature_name <= threshold value\n",
    "            split_question = f\"{feature_name_as_string} <= {split_threshold}\"\n",
    "\n",
    "            # create a dictionary for these split_questions \n",
    "            subtree = {split_question: []}\n",
    "\n",
    "            # recursion on our true values\n",
    "            answer_true = self.decision_tree(data_below_threshold, min_samples, max_depth, counter)\n",
    "\n",
    "            # recursion on our false values\n",
    "            answer_false = self.decision_tree(data_above_threshold, min_samples, max_depth, counter)\n",
    "\n",
    "            # if both answers are the same class\n",
    "            if answer_true == answer_false:\n",
    "\n",
    "                # choose one to be the subtree\n",
    "                subtree = answer_true\n",
    "            \n",
    "            # if answers result in different class\n",
    "            else:\n",
    "\n",
    "                # append to dictionary\n",
    "                subtree[split_question].append(answer_true)\n",
    "                subtree[split_question].append(answer_false)\n",
    "            \n",
    "            return subtree\n",
    "\n",
    "    def classify_observation(self, observation, tree):\n",
    "\n",
    "        # store the current question \n",
    "        split_question = list(tree.keys())[0]\n",
    "\n",
    "        # grab the feature name and value \n",
    "        feature_name, _, value = split_question.split()\n",
    "\n",
    "        # if the row at that feature column is less than the threshold\n",
    "        if observation[feature_name] <= float(value):\n",
    "\n",
    "            # answer yes, it's under the threshold\n",
    "            answer = tree[split_question][0]\n",
    "\n",
    "        # if the row at that feature column has exceeded the threshold\n",
    "        else:\n",
    "\n",
    "            # answer no, it has exceeded the threshold\n",
    "            answer = tree[split_question][1]\n",
    "        \n",
    "\n",
    "        # if the answer is not a dictionary\n",
    "        if not isinstance(answer, dict):\n",
    "\n",
    "            # return answer as it is a class label\n",
    "            return answer\n",
    "\n",
    "        # if the answer is a dictionary\n",
    "        else:\n",
    "\n",
    "            # recursion with the 'answer' subtree as the tree argument\n",
    "            return self.classify_observation(observation, answer)\n",
    "        \n",
    "    def predict(self, test_df):\n",
    "\n",
    "        # if a tree has been fitted\n",
    "        if self.tree:\n",
    "            \n",
    "            # store true labels for our test_df\n",
    "            y_test_true = test_df.iloc[:, -1]\n",
    "            \n",
    "            # create a new column for our predictions\n",
    "            test_df['predictions'] = test_df.apply(self.classify_observation, axis=1, args=(self.tree,))\n",
    "            \n",
    "            # return True or False where our predictions are equal to our labels\n",
    "            test_df['correct_predictions'] = test_df['predictions'] == y_test_true\n",
    "\n",
    "            # return how accurate the predictions are\n",
    "            return (\"Accuracy:\", test_df['correct_predictions'].mean())\n",
    "        \n",
    "        # if a tree wasn't fitted\n",
    "        else:\n",
    "            print(\"NotFittedError: Please fit your decision tree first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 5.1, 3.5, 1.4, 0.2, 'Iris-setosa'],\n",
       "       [2, 4.9, 3.0, 1.4, 0.2, 'Iris-setosa'],\n",
       "       [3, 4.7, 3.2, 1.3, 0.2, 'Iris-setosa'],\n",
       "       [4, 4.6, 3.1, 1.5, 0.2, 'Iris-setosa'],\n",
       "       [5, 5.0, 3.6, 1.4, 0.2, 'Iris-setosa']], dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_df.values\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31707317 0.34146341 0.34146341]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5840970252657152"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_entropy(train_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_splits = find_potential_splits(train_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01287312049569506\n",
      "0 4.35\n",
      "0.03856807071294372\n",
      "0.05137767157946454\n",
      "0.10219673763065067\n",
      "0.1272351322978637\n",
      "0.1761418903558301\n",
      "0.32803556179607246\n",
      "0.4847610605285572\n",
      "0.5811252011724818\n",
      "0.6306854382727127\n",
      "0.642113942702162\n",
      "0.7594692874469247\n",
      "0.8919877111004573\n",
      "1.0396058320300883\n",
      "1.1447485765438545\n",
      "1.2169038848912836\n",
      "1.3077703605240751\n",
      "1.3698286921579919\n",
      "1.4508397229358405\n",
      "1.6176204413295296\n",
      "1.780373089309827\n",
      "1.905168724426641\n",
      "1.878078041889502\n",
      "1.9313455927223449\n",
      "1.8765985183052571\n",
      "1.83623654537148\n",
      "1.447741499679517\n",
      "1.462410060029398\n",
      "1.5049378361544672\n",
      "1.5186511138543943\n",
      "1.5321467482150686\n",
      "1.5714008512276023\n",
      "0.012880334846127888\n",
      "0.038635806228670515\n",
      "0.12370393348864778\n",
      "0.1664243050738268\n",
      "0.36803281538650323\n",
      "0.475869376546437\n",
      "0.6239572042405866\n",
      "0.8697110773571067\n",
      "0.9829601856912552\n",
      "1.4563183038711567\n",
      "1.6760184733352728\n",
      "1.8117407553626144\n",
      "1.8008257510479153\n",
      "1.83445289988662\n",
      "2.0506017750570713\n",
      "2.0526413222891575\n",
      "2.1684091041021207\n",
      "1.5427228285177488\n",
      "1.5567318184434056\n",
      "1.570520700474797\n",
      "0.01287312049569506\n",
      "0.025731104437081088\n",
      "0.03856807071294372\n",
      "0.1272351322978637\n",
      "0.2338647279733192\n",
      "0.3190055970256217\n",
      "0.3471620559529852\n",
      "0.34175192848040165\n",
      "0.3170731707317073\n",
      "0.3800182825740779\n",
      "0.46788771622680486\n",
      "0.5394100727295134\n",
      "0.5715501068187187\n",
      "0.6018460871262737\n",
      "0.6839297900022557\n",
      "0.7772887004350639\n",
      "0.8371140507251842\n",
      "0.8884557215469178\n",
      "0.9176740428263889\n",
      "0.9621594517155918\n",
      "1.0550781227005854\n",
      "1.0529513313800423\n",
      "1.0119457083340804\n",
      "1.0663897219031369\n",
      "1.0638207013184007\n",
      "1.065251226842979\n",
      "1.0773512287159142\n",
      "1.1237828909780323\n",
      "1.1673457512180143\n",
      "1.2084523338990578\n",
      "1.2474209720963287\n",
      "1.3538009478123474\n",
      "1.3863263031767192\n",
      "1.4328101411066905\n",
      "1.462410060029398\n",
      "1.4909989556860888\n",
      "1.5049378361544672\n",
      "1.5186511138543943\n",
      "1.5321467482150686\n",
      "1.5454322491468206\n",
      "1.5714008512276023\n",
      "0.05137767157946454\n",
      "0.3190055970256217\n",
      "0.3452102280951781\n",
      "0.34175192848040165\n",
      "0.3338168828517431\n",
      "0.3170731707317073\n",
      "0.6018460871262737\n",
      "0.6839297900022557\n",
      "0.7981644784478437\n",
      "0.9424404494235199\n",
      "1.0547322736661777\n",
      "0.9906643993363398\n",
      "0.9340802401789878\n",
      "0.9153001942185837\n",
      "1.100956239264456\n",
      "1.1673457512180143\n",
      "1.2845043934757343\n",
      "1.3702265803488238\n",
      "1.4176050813355332\n",
      "1.5186511138543943\n",
      "1.5585147123772018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 4.35)"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_split(train_df.values, potential_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def entropy(data):\n",
    "\n",
    "        # labels for input data\n",
    "        labels = data[:,-1]\n",
    "\n",
    "        # get unique labels along with their counts\n",
    "        _, label_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "        # array of probabilities of each label\n",
    "        probabilities = label_counts / label_counts.sum()\n",
    "                \n",
    "        # return entropy\n",
    "        return sum(probabilities * -np.log2(probabilities))\n",
    "\n",
    "    def make_classification(data):\n",
    "\n",
    "        '''Once the max depth or min samples or purity is 1, \n",
    "        we classify the data with whatever the majority of the labels are'''\n",
    "\n",
    "        # labels for input data\n",
    "        labels = data[:,-1]\n",
    "\n",
    "        # instantiate a Counter object on the labels\n",
    "        counter = Counter(labels)\n",
    "\n",
    "        # return the most common class/label\n",
    "        return counter.most_common(1)[0][0]\n",
    "\n",
    "    def split_data(data, split_feature, split_threshold):\n",
    "\n",
    "        # array of only the split_feature\n",
    "        feature_values = data[:,split_feature]\n",
    "\n",
    "        # array where feature values do not exceed threshold, array where does exceed threshold\n",
    "        return data[feature_values <= split_threshold], data[feature_values > split_threshold]\n",
    "\n",
    "    def overall_entropy(data_below_threshold, data_above_threshold):\n",
    "\n",
    "        '''Overall entropy'''\n",
    "\n",
    "        p = len(data_below_threshold) / (len(data_below_threshold) + len(data_above_threshold))\n",
    "\n",
    "        return p * entropy(data_below_threshold) + p * entropy(data_above_threshold)\n",
    "\n",
    "    def find_potential_splits(data):\n",
    "        \n",
    "        # dictionary of splits\n",
    "        potential_splits = {}\n",
    "\n",
    "        # store the number of features (not including labels/target)\n",
    "        n_features = len(data[0])\n",
    "\n",
    "        # for each feature in possible features\n",
    "        for feature in range(n_features - 1):\n",
    "\n",
    "            # for our dictionary, each feature should be a key\n",
    "            potential_splits[feature] = []\n",
    "\n",
    "            # we need to iterate through each feature's unique values\n",
    "            unique_values_for_feature = np.unique(data[:, feature])\n",
    "\n",
    "            for index in range(len(unique_values_for_feature)):\n",
    "                \n",
    "                if index != 0:\n",
    "\n",
    "                    # we need to partition the data, we need the midpoint between the unique values\n",
    "                    current = unique_values_for_feature[index]\n",
    "                    prev = unique_values_for_feature[index - 1]\n",
    "                    midpoint = (current + prev) / 2\n",
    "\n",
    "                    # for our dictionary each value should be a midpoint between the \n",
    "                    # unique values for that feature\n",
    "                    potential_splits[feature].append(midpoint)\n",
    "\n",
    "        # return dictionary\n",
    "        return potential_splits\n",
    "\n",
    "    def find_best_split(data, potential_splits):\n",
    "\n",
    "        lowest_entropy = 9999\n",
    "\n",
    "        # for each dictionary key\n",
    "        for key in potential_splits:\n",
    "            \n",
    "            # for each value for that key\n",
    "            for value in potential_splits[key]:\n",
    "\n",
    "                # split our data into on that threshold (value)\n",
    "                data_below_threshold, data_above_threshold = split_data(\n",
    "                    data=data, \n",
    "                    split_feature=key,\n",
    "                    split_threshold=value)\n",
    "                \n",
    "                # calculate entropy at this split\n",
    "                entropy_for_this_split = overall_entropy(data_below_threshold, data_above_threshold)\n",
    "                \n",
    "                print(entropy_for_this_split)\n",
    "\n",
    "                # if entropy at this split is lower than the lowest entropy found so far\n",
    "                if entropy_for_this_split < lowest_entropy:\n",
    "\n",
    "                    # the entropy at this split is now the lowest \n",
    "                    lowest_entropy = entropy_for_this_split\n",
    "\n",
    "                    # keep a record of this key, value pair\n",
    "                    best_split_feature = key\n",
    "                    best_split_threshold = value\n",
    "                    \n",
    "                    print(best_split_feature, best_split_threshold)\n",
    "\n",
    "        # return the best potential split\n",
    "        return best_split_feature, best_split_threshold\n",
    "    \n",
    "\n",
    "    def purity(data):\n",
    "\n",
    "        # last column of the df must be the labels!\n",
    "        labels = data[:, -1]\n",
    "\n",
    "        # if data has only one kind of label\n",
    "        if len(np.unique(labels)) == 1:\n",
    "\n",
    "            # it is pure\n",
    "            return True\n",
    "\n",
    "        # if data has a few different labels still\n",
    "        else:\n",
    "\n",
    "            # it isn't pure\n",
    "            return False\n",
    "\n",
    "\n",
    "    def decision_tree(train_df, min_samples, max_depth, counter=0):\n",
    "\n",
    "        '''only one arg needed (df). fitting this training df will account for\n",
    "        splitting data into X and y'''\n",
    "\n",
    "        # if this is our first potential split\n",
    "        if counter == 0:\n",
    "\n",
    "            # set this global variable so we can use it each time we call decision_tree()\n",
    "            global feature_names\n",
    "            feature_names = train_df.columns\n",
    "\n",
    "            # get our df values\n",
    "            data = train_df.values\n",
    "\n",
    "        # if we have recursively reached this point\n",
    "        else:\n",
    "\n",
    "            # our 'train_df' is actually already an array upon recursion\n",
    "            data = train_df\n",
    "            \n",
    "        # base case: if our impurity for a subtree is 0 or we have reached our \n",
    "        # maximum depth or min samples\n",
    "        if (purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "            \n",
    "            # at this point we'll have to make a judgment call and classify it based on the majority\n",
    "            return make_classification(data)\n",
    "        \n",
    "\n",
    "        # if we haven't reach one of our stopping points\n",
    "        else:\n",
    "\n",
    "            # increment counter\n",
    "            counter += 1\n",
    "\n",
    "            # get a dictionary of our potential splits\n",
    "            potential_splits = find_potential_splits(data)\n",
    "            \n",
    "            \n",
    "            # find the best split\n",
    "            split_feature, split_threshold = find_best_split(data, potential_splits)\n",
    "            \n",
    "            print('Best Split:', split_feature, split_threshold)\n",
    "\n",
    "            # get the data below and above\n",
    "            data_below_threshold, data_above_threshold = split_data(data, split_feature, split_threshold)\n",
    "\n",
    "            # store feature name as string\n",
    "            feature_name_as_string = feature_names[split_feature]\n",
    "\n",
    "            # feature_name <= threshold value\n",
    "            split_question = f\"{feature_name_as_string} <= {split_threshold}\"\n",
    "\n",
    "            # create a dictionary for these split_questions \n",
    "            subtree = {split_question: []}\n",
    "\n",
    "            # recursion on our true values\n",
    "            answer_true = decision_tree(data_below_threshold, min_samples, max_depth, counter)\n",
    "\n",
    "            # recursion on our false values\n",
    "            answer_false = decision_tree(data_above_threshold, min_samples, max_depth, counter)\n",
    "\n",
    "            # if both answers are the same class\n",
    "            if answer_true == answer_false:\n",
    "\n",
    "                # choose one to be the subtree\n",
    "                subtree = answer_true\n",
    "            \n",
    "            # if answers result in different class\n",
    "            else:\n",
    "\n",
    "                # append to dictionary\n",
    "                subtree[split_question].append(answer_true)\n",
    "                subtree[split_question].append(answer_false)\n",
    "            \n",
    "            return subtree\n",
    "\n",
    "        \n",
    "    def predict(test_df):\n",
    "\n",
    "            \n",
    "        # store true labels for our test_df\n",
    "        y_test_true = test_df.iloc[:, -1]\n",
    "\n",
    "        # create a new column for our predictions\n",
    "        test_df['predictions'] = test_df.apply(self.classify_observation, axis=1, args=(self.tree,))\n",
    "\n",
    "        # return True or False where our predictions are equal to our labels\n",
    "        test_df['correct_predictions'] = test_df['predictions'] == y_test_true\n",
    "\n",
    "        # return how accurate the predictions are\n",
    "        return (\"Accuracy:\", test_df['correct_predictions'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def classify_observation(observation, tree):\n",
    "\n",
    "        # store the current question \n",
    "        split_question = list(tree.keys())[0]\n",
    "        \n",
    "        print(tree.keys())\n",
    "\n",
    "        # grab the feature name and value \n",
    "        feature_name, _, value = split_question.split()\n",
    "\n",
    "#         # if the row at that feature column is less than the threshold\n",
    "#         if observation[feature_name] <= float(value):\n",
    "\n",
    "#             # answer yes, it's under the threshold\n",
    "#             answer = tree[split_question][0]\n",
    "\n",
    "#         # if the row at that feature column has exceeded the threshold\n",
    "#         else:\n",
    "\n",
    "#             # answer no, it has exceeded the threshold\n",
    "#             answer = tree[split_question][1]\n",
    "\n",
    "#         # if the answer is not a dictionary\n",
    "#         if not isinstance(answer, dict):\n",
    "\n",
    "#             # return answer as it is a class label\n",
    "#             return answer\n",
    "\n",
    "#         # if the answer is a dictionary\n",
    "#         else:\n",
    "\n",
    "#             # recursion with the 'answer' subtree as the tree argument\n",
    "#             return classify_observation(observation, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.35\n",
      "Best Split: 0 4.35\n",
      "0 4.45\n",
      "1 2.1\n",
      "2 1.1\n",
      "Best Split: 2 1.1\n",
      "0 4.45\n",
      "1 2.1\n",
      "2 1.25\n",
      "Best Split: 2 1.25\n",
      "0 4.45\n",
      "1 2.1\n",
      "Best Split: 1 2.1\n",
      "0 4.45\n",
      "Best Split: 0 4.45\n"
     ]
    }
   ],
   "source": [
    "tree = decision_tree(train_df, min_samples=2, max_depth=5, counter=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SepalLengthCm <= 4.35': ['Iris-setosa', 'Iris-versicolor']}"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm         Species\n",
       "145            6.7           3.0            5.2           2.3  Iris-virginica\n",
       "146            6.3           2.5            5.0           1.9  Iris-virginica\n",
       "147            6.5           3.0            5.2           2.0  Iris-virginica\n",
       "148            6.2           3.4            5.4           2.3  Iris-virginica\n",
       "149            5.9           3.0            5.1           1.8  Iris-virginica"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SepalLengthCm                      5.4\n",
      "SepalWidthCm                         3\n",
      "PetalLengthCm                      4.5\n",
      "PetalWidthCm                       1.5\n",
      "Species                Iris-versicolor\n",
      "predictions                Iris-setosa\n",
      "correct_predictions              False\n",
      "Name: 84, dtype: object\n"
     ]
    }
   ],
   "source": [
    "obs = test_df.iloc[10]\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['SepalLengthCm <= 4.35'])\n"
     ]
    }
   ],
   "source": [
    "classify_observation(obs, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
