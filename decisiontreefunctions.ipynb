{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: we can't actually use pandas in our algorithm, but we can use it to read in our df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binary recursive partitioning. This is an iterative process of splitting the data into partitions, and then splitting it up further on each of the branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/decision-tree-classification-de64fc4d5aac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/what-is-a-decision-tree-22975f00f3e1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying splits with impurity\n",
    "https://victorzhou.com/blog/gini-impurity/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Gini Impurity is the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier\n",
    "Using the decision algorithm, **we start at the tree root and split the data on the feature that results in the largest information gain (IG)** (reduction in uncertainty towards the final decision).\n",
    "In an iterative process, we can then repeat this splitting procedure at each child node **until the leaves are pure**. This means that the samples at each leaf node all belong to the same class.\n",
    "In practice, we may **set a limit on the depth of the tree** to prevent overfitting. We compromise on purity here somewhat as the final leaves may still have some impurity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='Id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size):\n",
    "    \n",
    "    # get a list of indices\n",
    "    indices = df.index.to_list()\n",
    "    \n",
    "    # if a float result, rounding test_size to an int is needed\n",
    "    test_size = round(test_size * len(df))\n",
    "    \n",
    "    # get a random choice of the indices\n",
    "    test_indices = np.random.default_rng().choice(indices, size=test_size)\n",
    "\n",
    "    # create our test df\n",
    "    test = df.loc[test_indices]\n",
    "        \n",
    "    # create our train df \n",
    "    train = df.drop(test_indices)\n",
    "    \n",
    "    # return both\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if percentages work \n",
    "train, test = train_test_split(df, 0.20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy array \n",
    "train_data = train.values\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity(data):\n",
    "    \n",
    "    # get our labels for this data \n",
    "    labels = data[:,-1]\n",
    "    unique_classes = np.unique(labels)\n",
    "    \n",
    "    # see if we only have one label left\n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [np.unique(labels)[2]] * len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data):\n",
    "    \n",
    "    # get the true labels for the input data\n",
    "    labels = data[:,-1]\n",
    "    \n",
    "    # get the unique labels and label counts for this input data\n",
    "    classes, class_counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    # get the majority class\n",
    "    index = class_counts.argmax()\n",
    "    \n",
    "    # classify our input as the majority\n",
    "    classification = classes[index]\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data):\n",
    "    \n",
    "    label_column = data[:, -1]\n",
    "    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    index = counts_unique_classes.argmax()\n",
    "    classification = unique_classes[index]\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = classify(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(train[train.PetalWidthCm > 1.2].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_potential_splits(data):\n",
    "    \n",
    "    potential_splits = {}\n",
    "\n",
    "    # how many columns are there\n",
    "    _, n_columns = data.shape\n",
    "    \n",
    "    # for each column in our array \n",
    "    for column_index in range(n_columns - 1):\n",
    "    \n",
    "        # we want our key to be the column index\n",
    "        potential_splits[column_index] = []\n",
    "    \n",
    "        # we want our value to be each row value at that index\n",
    "        values = data[:, column_index]\n",
    "    \n",
    "        # we want all the possible splits\n",
    "        unique_values = np.unique(values)\n",
    "\n",
    "        # for each index in range(15)\n",
    "        for index in range(len(unique_values)):\n",
    "\n",
    "            if index != 0:\n",
    "\n",
    "                # get the sum of the current and previous values and \n",
    "                current_value = unique_values[index]\n",
    "                previous_value = unique_values[index-1]\n",
    "                potential_split = (current_value + previous_value) / 2\n",
    "\n",
    "                potential_splits[column_index].append(potential_split)\n",
    "    \n",
    "    return potential_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_splits = get_potential_splits(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one for each column besides the label\n",
    "potential_splits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=train, x='PetalWidthCm', y='PetalLengthCm', hue='Species', fit_reg=False)\n",
    "plt.hlines(y=potential_splits[2], xmin=0, xmax=2.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which question is most important?\n",
    "# ex. \"is the petal width less/larger than value\"\n",
    "# we have four questions to ask it\n",
    "\n",
    "# split dataset based on column and value\n",
    "def split_data(data, split_col, split_val):\n",
    "    \n",
    "    # get that specific column's values\n",
    "    split_column_values = data[:, split_col]\n",
    "    \n",
    "    # get the data below the specified value \n",
    "    data_below = data[split_column_values <= split_val]\n",
    "    \n",
    "    # get the data above the specified value\n",
    "    data_above = data[split_column_values > split_val]\n",
    "    \n",
    "    return data_below, data_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = 2\n",
    "split_val = 2.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: is our petal width < or > 0.8?\n",
    "data_below, data_above = split_data(train_data, split_col, split_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to calculate entropy for a split:\n",
    "\n",
    "Calculate entropy of parent node\n",
    "Calculate entropy of each individual node of split and calculate weighted average of all sub-nodes available in split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entropy = sum(probs * -log2(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    " def calculate_entropy(data):\n",
    "        \n",
    "    labels = data[:,-1]\n",
    "    _, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    probs = counts / counts.sum()\n",
    "\n",
    "    entropy = sum(probs * -np.log2(probs))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_entropy(data_below, data_above):\n",
    "    \n",
    "    total_data_points = len(data_below) + len(data_above)\n",
    "    \n",
    "    # weight of data below\n",
    "    p_of_data_below = len(data_below) / total_data_points\n",
    "    \n",
    "    # weight of data above\n",
    "    p_of_data_above = len(data_above) / total_data_points\n",
    "    \n",
    "    overall_entropy = (p_of_data_below * calculate_entropy(data_below))+(p_of_data_above * calculate_entropy(data_above))\n",
    "    \n",
    "    return overall_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6430565257160958"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_overall_entropy(data_below, data_above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_splits = get_potential_splits(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(data, potential_splits):\n",
    "    \n",
    "    '''Figure out where the best split is for given data'''\n",
    "    \n",
    "    # set entropy to abritrary high number\n",
    "    overall_entropy = 500\n",
    "    \n",
    "    # for each dictionary key (0,1,2,3)\n",
    "    for column_index in potential_splits:\n",
    "        \n",
    "        # for each value within that key\n",
    "        for value in potential_splits[column_index]:\n",
    "            \n",
    "            # split the data \n",
    "            data_below, data_above = split_data(train_data, split_col=column_index, split_val=value)\n",
    "            \n",
    "            # calculate the entropy\n",
    "            current_overall_entropy = calculate_overall_entropy(data_below, data_above)\n",
    "\n",
    "            if current_overall_entropy < overall_entropy:\n",
    "                overall_entropy = current_overall_entropy\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    \n",
    "    \n",
    "    return best_split_column, best_split_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_split(train_data, potential_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must be a recursive function\n",
    "def decision_tree(df, counter=0, min_samples=2, max_depth=5):\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        data = df.values \n",
    "    else:\n",
    "        data = df\n",
    "    \n",
    "    # base case\n",
    "    if (purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "        classification = classify(data)\n",
    "        return classification\n",
    "    \n",
    "    # if data not pure\n",
    "    else:\n",
    "        # increment counter\n",
    "        counter += 1\n",
    "        # recursion\n",
    "        potential_splits = get_potential_splits(data)\n",
    "        # Question: is our petal width < or > 0.8?\n",
    "    \n",
    "        split_col, split_val = find_best_split(data, potential_splits)\n",
    "        \n",
    "        data_below, data_above = split_data(data, split_col, split_val)\n",
    "        \n",
    "        feature_name = COLUMN_HEADERS[split_col]\n",
    "        question = \"{} <= {}\".format(feature_name, split_val)\n",
    "        subtree = {question: []}\n",
    "        \n",
    "        yes_answer = decision_tree(data_below, counter, min_samples, max_depth) \n",
    "        no_answer = decision_tree(data_above, counter, min_samples, max_depth)\n",
    "        \n",
    "        if yes_answer == no_answer:\n",
    "            subtree = yes_answer\n",
    "        else:\n",
    "        \n",
    "            subtree[question].append(yes_answer)\n",
    "            subtree[question].append(no_answer)\n",
    "        \n",
    "        return subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = decision_tree(train, max_depth=2)\n",
    "pprint.pprint(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PetalLengthCm <= 2.45': ['Iris-setosa', 'Iris-versicolor']}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_example(example, tree):\n",
    "    \n",
    "    # find the current question\n",
    "    question = list(tree.keys())[0]\n",
    "    # split it into the feature and value\n",
    "    feature_name, operator, value = question.split()\n",
    "    \n",
    "    # our tree is made up on subtrees\n",
    "    if example[feature_name] <= float(value):\n",
    "        # yes_answer = class \"Iris-setosa\"\n",
    "        answer = tree[question][0]\n",
    "    else:\n",
    "        # no answer = subtree\n",
    "        answer = tree[question][1]\n",
    "    \n",
    "    if not isinstance(answer, dict):\n",
    "            # return class\n",
    "        return answer\n",
    "    else:\n",
    "        # classify subtree\n",
    "        return classify_example(example, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_example(example, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df, tree):\n",
    "    \n",
    "    # new column for our predictions\n",
    "    df['predictions'] = df.apply(classify_example, axis=1, args=(tree,))\n",
    "    #calculate how acc\n",
    "    df['correct_predictions'] = df.predictions == df.Species\n",
    "    \n",
    "    accuracy = df['correct_predictions'].mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(train, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if percentages work \n",
    "train, test = train_test_split(df, 0.20)\n",
    "tree = decision_tree(train, max_depth=5)\n",
    "acc = calculate_accuracy(test, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
